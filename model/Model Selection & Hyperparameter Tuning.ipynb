{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646563ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train = pd.read_csv('https://raw.githubusercontent.com/vanilladucky/Housing-Prediction/main/data/cleaned/cleaned_train.csv')\n",
    "target = pd.read_csv('https://raw.githubusercontent.com/vanilladucky/Housing-Prediction/main/data/cleaned/cleaned_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c591939",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a90130",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58034dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>60</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>20</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>119.0</td>\n",
       "      <td>790</td>\n",
       "      <td>163</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>70</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>20</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>20</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>830</td>\n",
       "      <td>290</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0             60         65.0     8450            7            5       2003   \n",
       "1             20         80.0     9600            6            8       1976   \n",
       "2             60         68.0    11250            7            5       2001   \n",
       "3             70         60.0     9550            7            5       1915   \n",
       "4             60         84.0    14260            8            5       2000   \n",
       "...          ...          ...      ...          ...          ...        ...   \n",
       "1455          60         62.0     7917            6            5       1999   \n",
       "1456          20         85.0    13175            6            6       1978   \n",
       "1457          70         66.0     9042            7            9       1941   \n",
       "1458          20         68.0     9717            5            6       1950   \n",
       "1459          20         75.0     9937            5            6       1965   \n",
       "\n",
       "      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  GarageFinish  \\\n",
       "0             2003       196.0         706           0  ...             1   \n",
       "1             1976         0.0         978           0  ...             1   \n",
       "2             2002       162.0         486           0  ...             1   \n",
       "3             1970         0.0         216           0  ...             2   \n",
       "4             2000       350.0         655           0  ...             1   \n",
       "...            ...         ...         ...         ...  ...           ...   \n",
       "1455          2000         0.0           0           0  ...             1   \n",
       "1456          1988       119.0         790         163  ...             2   \n",
       "1457          2006         0.0         275           0  ...             1   \n",
       "1458          1996         0.0          49        1029  ...             2   \n",
       "1459          1965         0.0         830         290  ...             0   \n",
       "\n",
       "      GarageQual  GarageCond  PavedDrive  PoolQC  Fence  MiscFeature  \\\n",
       "0              4           4           2       3      4            4   \n",
       "1              4           4           2       3      4            4   \n",
       "2              4           4           2       3      4            4   \n",
       "3              4           4           2       3      4            4   \n",
       "4              4           4           2       3      4            4   \n",
       "...          ...         ...         ...     ...    ...          ...   \n",
       "1455           4           4           2       3      4            4   \n",
       "1456           4           4           2       3      2            4   \n",
       "1457           4           4           2       3      0            2   \n",
       "1458           4           4           2       3      4            4   \n",
       "1459           4           4           2       3      4            4   \n",
       "\n",
       "      SaleType  SaleCondition  SalePrice  \n",
       "0            8              4     208500  \n",
       "1            8              4     181500  \n",
       "2            8              4     223500  \n",
       "3            8              0     140000  \n",
       "4            8              4     250000  \n",
       "...        ...            ...        ...  \n",
       "1455         8              4     175000  \n",
       "1456         8              4     210000  \n",
       "1457         8              4     266500  \n",
       "1458         8              4     142125  \n",
       "1459         8              4     147500  \n",
       "\n",
       "[1460 rows x 80 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.join(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8a6fd",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c45662a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb7d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3baba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 79)\n",
      "(1, 1460)\n"
     ]
    }
   ],
   "source": [
    "np_train = np.array(train)\n",
    "np_target = np.array(target).reshape(1,-1)\n",
    "print(np_train.shape)\n",
    "print(np_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344f4eb",
   "metadata": {},
   "source": [
    "### Below code for cross validation to be used within the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "543cd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index , test_index in kf.split(train):\n",
    "    X_train , X_test = train.iloc[train_index,:],train.iloc[test_index,:]\n",
    "    y_train , y_test = target.iloc[train_index,:] , target.iloc[test_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ebd1c",
   "metadata": {},
   "source": [
    "# Simple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dc5b1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of each fold -> [34151.938760528064, 46451.54860636613, 51320.22220864116, 30968.183863439008, 40346.456686016914]\n",
      "Avg error : 40647.67002499825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "acc_score = []\n",
    "# Cross Validation\n",
    "for train_index , test_index in kf.split(train):\n",
    "    X_train , X_test = train.iloc[train_index,:],train.iloc[test_index,:]\n",
    "    y_train , y_test = target.iloc[train_index,:] , target.iloc[test_index,:]\n",
    "     \n",
    "    linear_regression.fit(X_train,y_train)\n",
    "    pred_values = linear_regression.predict(X_test)\n",
    "     \n",
    "    acc = mean_squared_error(pred_values , y_test, squared=False)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/5\n",
    " \n",
    "print('error of each fold -> {}'.format(acc_score))\n",
    "print('Avg error : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4b5f2",
   "metadata": {},
   "source": [
    "# Decision Trees for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb8ec84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of each fold -> [48734.156923747985, 42575.703054909885, 33311.136859205806, 39508.508095404686, 39250.46731346447]\n",
      "Avg error : 40675.99444934657\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "decision_tree = tree.DecisionTreeRegressor()\n",
    "\n",
    "acc_score = []\n",
    "# Cross Validation\n",
    "for train_index , test_index in kf.split(train):\n",
    "    X_train , X_test = train.iloc[train_index,:],train.iloc[test_index,:]\n",
    "    y_train , y_test = target.iloc[train_index,:] , target.iloc[test_index,:]\n",
    "     \n",
    "    decision_tree.fit(X_train,y_train)\n",
    "    pred_values = decision_tree.predict(X_test)\n",
    "     \n",
    "    acc = mean_squared_error(pred_values , y_test, squared=False)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/5\n",
    " \n",
    "print('error of each fold -> {}'.format(acc_score))\n",
    "print('Avg error : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fee8d3",
   "metadata": {},
   "source": [
    "# Random Forest for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c077ef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of each fold -> [32755.195494506104, 26554.531286095054, 29161.85950386168, 24859.583420906365, 32600.071728619077]\n",
      "Avg error: 29186.248286797654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfg = RandomForestRegressor(n_estimators = 500, max_depth=10, n_jobs=-1)\n",
    "\n",
    "acc_score = []\n",
    "# Cross Validation\n",
    "for train_index , test_index in kf.split(train):\n",
    "    X_train , X_test = train.iloc[train_index,:],train.iloc[test_index,:]\n",
    "    y_train , y_test = target.iloc[train_index,:] , target.iloc[test_index,:]\n",
    "     \n",
    "    rfg.fit(X_train,y_train)\n",
    "    pred_values = rfg.predict(X_test)\n",
    "     \n",
    "    acc = mean_squared_error(pred_values , y_test, squared=False)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/5\n",
    " \n",
    "print('error of each fold -> {}'.format(acc_score))\n",
    "print('Avg error: {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c61716",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6fd320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of each fold -> [25062.57655746836, 32747.244227322433, 28527.864830674953, 24986.762923552345, 34689.88026931846]\n",
      "Avg error : 29202.865761667308\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgboost = xgb.XGBRegressor(n_jobs=-1, n_estimators = 200)\n",
    "\n",
    "acc_score = []\n",
    "# Cross Validation\n",
    "for train_index , test_index in kf.split(train):\n",
    "    X_train , X_test = train.iloc[train_index,:],train.iloc[test_index,:]\n",
    "    y_train , y_test = target.iloc[train_index,:] , target.iloc[test_index,:]\n",
    "     \n",
    "    xgboost.fit(X_train,y_train)\n",
    "    pred_values = xgboost.predict(X_test)\n",
    "     \n",
    "    acc = mean_squared_error(pred_values , y_test, squared=False)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/5\n",
    " \n",
    "print('error of each fold -> {}'.format(acc_score))\n",
    "print('Avg error : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997840e8",
   "metadata": {},
   "source": [
    "# Now that we have observed Random Forest and XGBoost to yield the lowest error, we would now use these models and perform hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e033e58",
   "metadata": {},
   "source": [
    "## First, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46a709f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf98156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfg = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rfg, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 100, \n",
    "                               cv = 5, \n",
    "                               verbose=2, \n",
    "                               random_state=42, \n",
    "                               n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e27ba18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 400,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': None,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692923bc",
   "metadata": {},
   "source": [
    "### Now that we have a certain range, we would go in-depth by using Grid Search with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b38e7d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 300}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1,2,3],\n",
    "    'min_samples_split': [2,4,6],\n",
    "    'n_estimators': [300,350,400,450]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rfg, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 5)\n",
    "\n",
    "grid_search.fit(train, target)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d9912",
   "metadata": {},
   "source": [
    "### Let us check the error rate with these hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4079877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of each fold -> [24565.51985324077, 29888.8743057866, 30188.85646627657, 28676.913458268842, 27455.486971817772]\n",
      "Avg error: 28155.130211078114\n"
     ]
    }
   ],
   "source": [
    "tuned_rfg = RandomForestRegressor(n_estimators = 400, \n",
    "                                  max_depth=None, \n",
    "                                  bootstrap = False,\n",
    "                                  max_features='sqrt',\n",
    "                                  min_samples_leaf = 1,\n",
    "                                  min_samples_split = 2,\n",
    "                                  n_jobs=-1, )\n",
    "\n",
    "acc_score = []\n",
    "# Cross Validation\n",
    "for train_index , test_index in kf.split(train):\n",
    "    X_train , X_test = train.iloc[train_index,:],train.iloc[test_index,:]\n",
    "    y_train , y_test = target.iloc[train_index,:] , target.iloc[test_index,:]\n",
    "     \n",
    "    rfg.fit(X_train,y_train)\n",
    "    pred_values = rfg.predict(X_test)\n",
    "     \n",
    "    acc = mean_squared_error(pred_values , y_test, squared=False)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/5\n",
    " \n",
    "print('error of each fold -> {}'.format(acc_score))\n",
    "print('Avg error: {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09046d77",
   "metadata": {},
   "source": [
    "### As we can see, there is a very small reduction in the error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b15aa",
   "metadata": {},
   "source": [
    "## Now, let's see XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86e6a912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': [2, 3, 4, 5, 6, 7, 8, 9], 'eta': [0.1, 0.3, 0.5, 0.7, 0.9], 'gamma': [0, 1, 2, 3, 4, 5], 'min_child_weight': [0, 1, 2, 3, 4, 5], 'subsample': [0.1, 0.3, 0.5, 0.7, 0.9]}\n"
     ]
    }
   ],
   "source": [
    "max_depth = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "eta = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "gamma = [0, 1, 2, 3, 4, 5]\n",
    "min_child_weight = [0, 1, 2, 3, 4, 5]\n",
    "subsample = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "# Create the random grid\n",
    "random_grid = {'max_depth': max_depth,\n",
    "               'eta': eta,\n",
    "               'gamma': gamma,\n",
    "               'min_child_weight': min_child_weight,\n",
    "               'subsample': subsample,\n",
    "              }\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e01a6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.7,\n",
       " 'min_child_weight': 2,\n",
       " 'max_depth': 4,\n",
       " 'gamma': 4,\n",
       " 'eta': 0.1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost = xgb.XGBRegressor(tree_method = 'gpu_hist')\n",
    "xgboost_random = RandomizedSearchCV(estimator = xgboost, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 100, \n",
    "                               cv = 5, \n",
    "                               verbose=3, \n",
    "                               random_state=42, \n",
    "                               n_jobs = -1)\n",
    "# Fit the random search model\n",
    "xgboost_random.fit(train, target)\n",
    "\n",
    "xgboost_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e547a473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eta': 0.1,\n",
       " 'gamma': 4,\n",
       " 'max_depth': 4,\n",
       " 'min_child_weight': 2.5,\n",
       " 'subsample': 0.75}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'eta': [0.05, 0.1, 0.15],\n",
    "    'gamma': [4, 5, 6, 3],\n",
    "    'min_child_weight': [1.5, 2, 2.5],\n",
    "    'subsample': [0.6, 0.65, 0.7, 0.75, 0.8],\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = xgboost, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 5)\n",
    "\n",
    "grid_search.fit(train, target)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf69b8",
   "metadata": {},
   "source": [
    "### Let us check the error rate with these hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c525ad72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of each fold -> [27284.856471873674, 30354.858463046585, 23379.533937224962, 22750.13687459666, 33001.679101911104]\n",
      "Avg error : 27354.212969730597\n"
     ]
    }
   ],
   "source": [
    "xgboost = xgb.XGBRegressor(n_jobs=-1, \n",
    "                            eta = 0.1, \n",
    "                            gamma = 4, \n",
    "                            max_depth = 4,\n",
    "                            min_child_weight = 2.5, \n",
    "                            subsample=0.75)\n",
    "\n",
    "acc_score = []\n",
    "# Cross Validation\n",
    "for train_index , test_index in kf.split(train):\n",
    "    X_train , X_test = train.iloc[train_index,:],train.iloc[test_index,:]\n",
    "    y_train , y_test = target.iloc[train_index,:] , target.iloc[test_index,:]\n",
    "     \n",
    "    xgboost.fit(X_train,y_train)\n",
    "    pred_values = xgboost.predict(X_test)\n",
    "     \n",
    "    acc = mean_squared_error(pred_values , y_test, squared=False)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/5\n",
    " \n",
    "print('error of each fold -> {}'.format(acc_score))\n",
    "print('Avg error : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bfb445",
   "metadata": {},
   "source": [
    "### We observe that XGBoost also experienced a reducition in error which is great news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b678e6",
   "metadata": {},
   "source": [
    "# Let us now create a stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5350869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models = dict()\n",
    "    models['rf'] = RandomForestRegressor(n_estimators = 400, \n",
    "                                  max_depth=None, \n",
    "                                  bootstrap = False,\n",
    "                                  max_features='sqrt',\n",
    "                                  min_samples_leaf = 1,\n",
    "                                  min_samples_split = 2,\n",
    "                                  n_jobs=-1, )\n",
    "    models['xgb'] = xgb.XGBRegressor(n_jobs=-1, \n",
    "                            eta = 0.1, \n",
    "                            gamma = 4, \n",
    "                            max_depth = 4,\n",
    "                            min_child_weight = 2.5, \n",
    "                            subsample=0.75)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4693ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d477dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('rf', RandomForestRegressor(n_estimators = 400, \n",
    "                                  max_depth=None, \n",
    "                                  bootstrap = False,\n",
    "                                  max_features='sqrt',\n",
    "                                  min_samples_leaf = 1,\n",
    "                                  min_samples_split = 2,\n",
    "                                  n_jobs=-1)))\n",
    "    level0.append(('xgb', xgb.XGBRegressor(n_jobs=-1, \n",
    "                            eta = 0.1, \n",
    "                            gamma = 4, \n",
    "                            max_depth = 4,\n",
    "                            min_child_weight = 2.5, \n",
    "                            subsample=0.75)))\n",
    "    # define meta learner model\n",
    "    level1 = LinearRegression()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model\n",
    "\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['rf'] = RandomForestRegressor(n_estimators = 400, \n",
    "                                  max_depth=None, \n",
    "                                  bootstrap = False,\n",
    "                                  max_features='sqrt',\n",
    "                                  min_samples_leaf = 1,\n",
    "                                  min_samples_split = 2,\n",
    "                                  n_jobs=-1, )\n",
    "    models['xgb'] = xgb.XGBRegressor(n_jobs=-1, \n",
    "                            eta = 0.1, \n",
    "                            gamma = 4, \n",
    "                            max_depth = 4,\n",
    "                            min_child_weight = 2.5, \n",
    "                            subsample=0.75)\n",
    "    models['stacking'] = get_stacking()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47320aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">rf -776488851.604 (284898216.839)\n",
      ">xgb -731955065.145 (351753686.982)\n",
      ">stacking -687174990.825 (321993152.961)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, train, target)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11887da9",
   "metadata": {},
   "source": [
    "### Let's see our error value for our stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab65d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of each fold -> [22841.418906009072, 24937.62300218031, 21910.20576023874, 20922.633818493832, 33771.22200141746]\n",
      "Avg error: 24876.62069766788\n"
     ]
    }
   ],
   "source": [
    "# define the base models\n",
    "level0 = list()\n",
    "level0.append(('rf', RandomForestRegressor(n_estimators = 400, \n",
    "                                  max_depth=None, \n",
    "                                  bootstrap = False,\n",
    "                                  max_features='sqrt',\n",
    "                                  min_samples_leaf = 1,\n",
    "                                  min_samples_split = 2,\n",
    "                                  n_jobs=-1)))\n",
    "level0.append(('xgb', xgb.XGBRegressor(n_jobs=-1, \n",
    "                            eta = 0.1, \n",
    "                            gamma = 4, \n",
    "                            max_depth = 4,\n",
    "                            min_child_weight = 2.5, \n",
    "                            subsample=0.75)))\n",
    "# define meta learner model\n",
    "level1 = LinearRegression()\n",
    "# define the stacking ensemble\n",
    "stacked_model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "\n",
    "acc_score = []\n",
    "# Cross Validation\n",
    "for train_index , test_index in kf.split(train):\n",
    "    X_train , X_test = train.iloc[train_index,:],train.iloc[test_index,:]\n",
    "    y_train , y_test = target.iloc[train_index,:] , target.iloc[test_index,:]\n",
    "     \n",
    "    stacked_model.fit(X_train,y_train)\n",
    "    pred_values = stacked_model.predict(X_test)\n",
    "     \n",
    "    acc = mean_squared_error(pred_values , y_test, squared=False)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/5\n",
    " \n",
    "print('error of each fold -> {}'.format(acc_score))\n",
    "print('Avg error: {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06273139",
   "metadata": {},
   "source": [
    "## Althought it doesn't seem much different from XGBoost's tuned model, we would proceed to use this model for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49397114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(stacked_model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchcuda",
   "language": "python",
   "name": "pytorchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
